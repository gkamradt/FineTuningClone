{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ac59f3",
   "metadata": {},
   "source": [
    "# Method 5: Fine Tuning - Conversational Pairs\n",
    "\n",
    "In this series we are exploring how to match tone of a sample. My goal is to instruct and tune the LLM to talk like me. I'll use a few podcasts I've been on as examples.\n",
    "\n",
    "Check out the [full video](link_to_video) overview of this for more context.\n",
    "\n",
    "For this method we are going to try to reproduce my speaking style based on conversational pairs. This means I'll take my back and forth conversation w/ a podcast host as the training set for a fine tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72bdaaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853d40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model='gpt-4', openai_api_key=os.getenv(\"OPENAI_API_KEY\", \"YOUR_API_KEY_HERE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c89a3a",
   "metadata": {},
   "source": [
    "### Previous work\n",
    "We did a bunch of work in the previous methods to get my tone description, relevant docs to our sample query and writing examples. We'll load those up here so we don't need to run the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d80bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a text file of a bunch of lines that I said\n",
    "with open(\"Transcripts/GregLines.txt\", 'r') as file:\n",
    "    greg_lines = file.read()\n",
    "\n",
    "# This is a description of my tone as determined by the LLM (previous method)\n",
    "with open(\"gregs_tone_description.txt\", 'r') as file:\n",
    "    gregs_tone_description = file.read()\n",
    "\n",
    "# This are specific references to me talking about a previous role that I had\n",
    "with open(\"first_job_college_relevant_docs.txt\", 'r') as file:\n",
    "    relevant_docs = file.read()\n",
    "\n",
    "# These are specific examples of how I talk, similar to the GregLines above\n",
    "with open(\"greg_example_writing.txt\", 'r') as file:\n",
    "    writing_examples = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f337f3e",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff541bb",
   "metadata": {},
   "source": [
    "Now we are going to move onto the fun part, fine tuning. I want to use a open sourced model to save on costs and see how a model that *hasn't* been trained on so much safety does.\n",
    "\n",
    "To do this I'm going to fine tune and run my model via [Gradient.ai](https://gradient.ai/) who helped sponsor this video. Super easy to get set up and the team has been responsive.\n",
    "\n",
    "### Step 1: Create Conversational Pair\n",
    "\n",
    "When you fine tune it's recommended to have a set of validated 'input' and 'output' pairs. This is your training set. I'm going to use my transcripts as the output, but what do use for the input?\n",
    "\n",
    "I'm going to use the back and forth conversation of the podcasts I've been on. The question or statement that preceeds my response will be in input and my response will be the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3679c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = ['Transcripts/transcript_1.txt', 'Transcripts/transcript_2.txt', 'Transcripts/transcript_3.txt']\n",
    "\n",
    "result = []\n",
    "\n",
    "for transcript_file in transcripts:\n",
    "    transcript = open(transcript_file, 'r').read()\n",
    "    \n",
    "    # Split up the sections\n",
    "    sections = re.split(r'(\\d{2}:\\d{2}:\\d{2} [A-Za-z]+:)', transcript)\n",
    "    sections = [section.strip() for section in sections if section.strip()]  # Removing any empty strings\n",
    "\n",
    "    # Now, we'll pair up the speaker names with their corresponding content\n",
    "    paired_sections = [(sections[i], sections[i+1]) for i in range(0, len(sections), 2)]\n",
    "\n",
    "    # Extracting 'input' and 'output' pairs\n",
    "    \n",
    "    for i in range(1, len(paired_sections)):\n",
    "        if paired_sections[i][0].startswith(\"00:00:00 Greg:\"):\n",
    "            continue\n",
    "        if paired_sections[i][0].startswith(\"00:\") and \"Greg:\" in paired_sections[i][0]:\n",
    "            input_text = paired_sections[i-1][1]\n",
    "            output_text = paired_sections[i][1]\n",
    "            \n",
    "            # Remove long examples due to context limit\n",
    "            if len(input_text) > 1000 or len(output_text) > 1000:\n",
    "                continue\n",
    "            \n",
    "            result.append({'input': input_text, 'output': output_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4177be51",
   "metadata": {},
   "source": [
    "Great let's take a look at a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "418449cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'yeah,',\n",
       "  'output': 'you can you I mean, My my DMs are open on Twitter, and they will be for the foreseeable future. So, just hit me up there.'},\n",
       " {'input': 'Yes. However Right. Right.',\n",
       "  'output': \"The benchmarks all show that open source aren't as good as the closed models. I'm a capitalist at heart, and so I love market pressure. And so I love the fact that there are multiple model providers all battling for consumer value and consumer attention. Because as a consumer myself, I will gladly, love the benefit of them battling for my\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[31:33]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154db361",
   "metadata": {},
   "source": [
    "Let's see how many data points we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7119840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac85b8",
   "metadata": {},
   "source": [
    "Ok, so 67 data points to work with, let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc7bce",
   "metadata": {},
   "source": [
    "Great, now that we have our input/output pairs, let's transformt them into training data points. You can see what the suggested format is on [Gradient's website](https://docs.gradient.ai/docs/tips-and-tricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62860335",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "\n",
    "for pair in result:\n",
    "    training_set.append({\"inputs\": f\"<s>### Instruction:\\n{pair['input']}\\n\\n### Response:\\n{pair['output']}</s>\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c779e37",
   "metadata": {},
   "source": [
    "### Fine Tuning\n",
    "\n",
    "Now we are on to the fine tuning step. We'll use their Nous Hermes 2 model. You can check out their full list of supported models [here](https://docs.gradient.ai/docs/models-1). You'll need to use python 3.10+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d6b4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradientai import Gradient\n",
    "\n",
    "# Make your Gradient client\n",
    "gradient = Gradient(access_token=os.getenv(\"GRADIENT_API_TOKEN\", \"YourTokenHere\"),\n",
    "                    workspace_id=os.getenv(\"GRADIENT_WORKSPACE_ID\", \"YourWorkSpaceIdHere\"))\n",
    "\n",
    "# Get your base model ready. You'll need to grab the model slug from Gradient's website\n",
    "base_model = gradient.get_base_model(base_model_slug=\"nous-hermes2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b895309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model adapter with id d362b9c1-d793-4e0b-bd7a-dcd8002f37b8_model_adapter\n"
     ]
    }
   ],
   "source": [
    "# Create your new model which you'll stem from the base model\n",
    "new_model = base_model.create_model_adapter(\n",
    "    name=\"My Greg Model - Conversational Prompts\"\n",
    ")\n",
    "\n",
    "print(f\"Created model adapter with id {new_model.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921fd941",
   "metadata": {},
   "source": [
    "Great! Now let's do the cool part of fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6ef7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuneResponse(number_of_trainable_tokens=10113, sum_loss=32148.799)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training on the training_set we made above\n",
    "new_model.fine_tune(samples=training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0db708",
   "metadata": {},
   "source": [
    "### Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26e98e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GradientLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5325c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GradientLLM(\n",
    "    # `ID` listed in `$ gradient model list`\n",
    "    model=new_model.id,\n",
    "    # optional: set new credentials, they default to environment variables\n",
    "    gradient_workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
    "    gradient_access_token=os.environ[\"GRADIENT_API_TOKEN\"],\n",
    "    model_kwargs=dict(max_generated_token_count=228)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22a3a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What was your first job out of college? Did you like it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ade4440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeah. Yeah. Yeah. It's a good question. I would say that there's kind of 2 Pivotal pivotal moments. My 1st role out of school. So I'm a fresh fresh out of undergrad, very green in the workplace. And I started my career off in corporate FP and A. And that means I was doing finance and budgets for big corporations in their, their departments. Right? I remember getting an Excel spreadsheet. This was back before Google drive or links or anything like that. So it was just a big massive dirty un version controlled spreadsheet. And, there was a list of transactions throughout the entire page and, you know, call it something like a 100,000 transactions or whatever, more rows than I could count, more columns than I could see. And my boss was basically like, Greg, you tell us what tell us X, Y, Z. Like, I forgot what the question was, but they were like, go figure out the answer within the spreadsheet. And\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's load it all up into a good prompt for us to use!\n",
    "\n",
    "template = \"\"\"\n",
    "    <s>\n",
    "    You are a person named Greg Kamradt\n",
    "    \n",
    "    Here is background context to use when answering the question\n",
    "    **Start of relevant information**\n",
    "    {background_information}\n",
    "    **End of relevant information**\n",
    "    \n",
    "    Here are some examples of how Greg Kamradt talks, mimic the tone you see here\n",
    "    **Start of examples information**\n",
    "    {examples}\n",
    "    **End of examples information**\n",
    "    \n",
    "    % ANSWER THIS QUESTION\n",
    "    {question}\n",
    "    \n",
    "    % YOUR RESPONSE:\n",
    "    </s>\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"background_information\", \"question\", \"examples\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(\n",
    "    background_information = relevant_docs,\n",
    "    examples=writing_examples,\n",
    "    question = question\n",
    ")\n",
    "\n",
    "llm_answer = llm.predict(final_prompt)\n",
    "\n",
    "print (llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08e94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
