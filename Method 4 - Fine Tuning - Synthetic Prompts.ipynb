{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ac59f3",
   "metadata": {},
   "source": [
    "# Method 4: Fine Tuning - Synthetic Prompts\n",
    "\n",
    "In this series we are exploring how to match tone of a sample. My goal is to instruct and tune the LLM to talk like me. I'll use a few podcasts I've been on as examples.\n",
    "\n",
    "Check out the [full video](link_to_video) overview of this for more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72bdaaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853d40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model='gpt-4', openai_api_key=os.getenv(\"OPENAI_API_KEY\", \"YOUR_API_KEY_HERE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c89a3a",
   "metadata": {},
   "source": [
    "### Previous work\n",
    "We did a bunch of work in the previous methods to get my tone description, relevant docs to our sample query and writing examples. We'll load those up here so we don't need to run the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d80bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a text file of a bunch of lines that I said\n",
    "with open(\"Transcripts/GregLines.txt\", 'r') as file:\n",
    "    greg_lines = file.read()\n",
    "\n",
    "# This is a description of my tone as determined by the LLM (previous method)\n",
    "with open(\"gregs_tone_description.txt\", 'r') as file:\n",
    "    gregs_tone_description = file.read()\n",
    "\n",
    "# This are specific references to me talking about a previous role that I had\n",
    "with open(\"first_job_college_relevant_docs.txt\", 'r') as file:\n",
    "    relevant_docs = file.read()\n",
    "\n",
    "# These are specific examples of how I talk, similar to the GregLines above\n",
    "with open(\"greg_example_writing.txt\", 'r') as file:\n",
    "    writing_examples = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f337f3e",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff541bb",
   "metadata": {},
   "source": [
    "Now we are going to move onto the fun part, fine tuning. I want to use a open sourced model to save on costs and see how a model that *hasn't* been trained on so much safety does.\n",
    "\n",
    "To do this I'm going to fine tune and run my model via [Gradient.ai](https://gradient.ai/) who helped sponsor this video. Super easy to get set up and the team has been responsive.\n",
    "\n",
    "### Step 1: Create Synthetic Prompts\n",
    "\n",
    "When you fine tune it's recommended to have a set of validated 'input' and 'output' pairs. This is your training set. I'm going to use my transcripts as the output, but what do use for the input?\n",
    "\n",
    "I'm going to try having gpt4 generated me an synthetic 'input' that I'll use for my training set. Kind of like [Jeopardy](https://www.jeopardy.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3679c09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greg_lines_list = greg_lines.split(\"\\n\\n\")\n",
    "greg_lines_list = [x for x in greg_lines_list if len(x) < 1500 and len(x) > 100]\n",
    "\n",
    "len(greg_lines_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154db361",
   "metadata": {},
   "source": [
    "Looks like I have 60 lines after some filtering. Let's start there. It may seem like not that many data points but I want to try it out.\n",
    "\n",
    "Now we'll have GPT4 generate us some inputs that would have resulted in these outputs. I'll save each pair in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4704fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below if you want to run it manually or else skip below and load the file I've already done\n",
    "\n",
    "# input_pairs = []\n",
    "\n",
    "# for i, line in enumerate(greg_lines_list):\n",
    "#     # Status counter\n",
    "#     if i % 10 == 0:\n",
    "#         print (i)\n",
    "    \n",
    "#     # Finally, let's load it all up into a good prompt for us to use!\n",
    "#     template = \"\"\"\n",
    "#         You are a bot that is good at generating an 'input' statement given a statement someone says\n",
    "        \n",
    "#         Your goal is to ask a question that would have resulted in the output statement you're given\n",
    "        \n",
    "#         Think of it like a game of of Jeopardy.\n",
    "        \n",
    "#         -Example-\n",
    "#         Output: Last night I went to dinner and had a great time with my wife!\n",
    "#         Input: What did you do last night?\n",
    "#         -End Of Examples-\n",
    "        \n",
    "#         Here is the output you should give an input to: {greg_line}\n",
    "#         \"\"\"\n",
    "\n",
    "#     prompt = PromptTemplate(\n",
    "#         input_variables=[\"greg_line\"],\n",
    "#         template=template,\n",
    "#     )\n",
    "\n",
    "#     final_prompt = prompt.format(\n",
    "#         greg_line = line\n",
    "#     )\n",
    "\n",
    "#     llm_answer = chat.predict(final_prompt)\n",
    "\n",
    "#     input_pairs.append({\n",
    "#         'input' : llm_answer,\n",
    "#         'output' : line\n",
    "#     })\n",
    "\n",
    "# with open(\"greg_synthetic_pairs.json\", \"w\") as file:\n",
    "#     json.dump(input_pairs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c19b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"greg_synthetic_pairs.json\", \"r\") as file:\n",
    "    input_pairs = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84873349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What are your predictions for the development of OpenAI and other open source models in the next 18 months?',\n",
       " 'output': \"However, in that 18 months when when those open source models are getting better, OpenAI is gonna improve their capabilities. They're gonna come out with good stuff, and they're gonna be on a GPT 5, 6, whatever.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see a sample\n",
    "input_pairs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac85b8",
   "metadata": {},
   "source": [
    "I'll save this to file so we don't have to do it again or you can load it up on your own if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc7bce",
   "metadata": {},
   "source": [
    "Great, now that we have our input/output pairs, let's transformt them into training data points. You can see what the suggested format is on [Gradient's website](https://docs.gradient.ai/docs/tips-and-tricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62860335",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "\n",
    "for pair in input_pairs:\n",
    "    training_set.append({\"inputs\": f\"<s>### Instruction:\\n{pair['input']}\\n\\n### Response:\\n{pair['output']}</s>\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c779e37",
   "metadata": {},
   "source": [
    "### Fine Tuning\n",
    "\n",
    "Now we are on to the fine tuning step. We'll use their Nous Hermes 2 model. You can check out their full list of supported models [here](https://docs.gradient.ai/docs/models-1). You'll need to use python 3.10+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6b4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradientai import Gradient\n",
    "\n",
    "# Make your Gradient client\n",
    "gradient = Gradient(access_token=os.getenv(\"GRADIENT_API_TOKEN\", \"YourTokenHere\"),\n",
    "                    workspace_id=os.getenv(\"GRADIENT_WORKSPACE_ID\", \"YourWorkSpaceIdHere\"))\n",
    "\n",
    "# Get your base model ready. You'll need to grab the model slug from Gradient's website\n",
    "base_model = gradient.get_base_model(base_model_slug=\"nous-hermes2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdf2b4b9-a812-438c-8fd4-aa0498a8da46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model adapter with id 7d314527-d693-4ecf-8d7c-3bc6876ed436_model_adapter\n"
     ]
    }
   ],
   "source": [
    "### Create your new model which you'll stem from the base model\n",
    "new_model = base_model.create_model_adapter(\n",
    "    name=\"My Greg Model - Synthetic Prompts\"\n",
    ")\n",
    "\n",
    "print(f\"Created model adapter with id {new_model.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921fd941",
   "metadata": {},
   "source": [
    "Great! Now let's do the cool part of fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff6ef7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuneResponse(number_of_trainable_tokens=12433, sum_loss=35208.676)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training on the training_set we made above\n",
    "new_model.fine_tune(samples=training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0db708",
   "metadata": {},
   "source": [
    "### Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e98e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GradientLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5325c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GradientLLM(\n",
    "    # `ID` listed in `$ gradient model list`\n",
    "    model=new_model.id,\n",
    "    # optional: set new credentials, they default to environment variables\n",
    "    gradient_workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
    "    gradient_access_token=os.environ[\"GRADIENT_API_TOKEN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22a3a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what was your first job out of college? Did you like it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ade4440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Start of answer**\n",
      "    Yeah. Yeah. Yeah. It's a good question.\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's load it all up into a good prompt for us to use!\n",
    "\n",
    "template = \"\"\"\n",
    "    <s>\n",
    "    Speak in the tone & style of Greg Kamradt.\n",
    "    Respond in a short, conversational manner that answers the question below.\n",
    "    \n",
    "    Here is relevant information that can be used to answer the question\n",
    "    **Start of relevant information**\n",
    "    {background_information}\n",
    "    **End of relevant information**\n",
    "    \n",
    "    Here are some examples of how Greg Kamradt talks, mimic the tone you see here\n",
    "    **Start of examples information**\n",
    "    {examples}\n",
    "    **End of examples information**\n",
    "    \n",
    "    ANSWER THIS QUESTION: {question}\n",
    "    </s>\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"background_information\", \"question\", \"examples\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(\n",
    "    background_information = relevant_docs,\n",
    "    examples=writing_examples,\n",
    "    question = question\n",
    ")\n",
    "\n",
    "llm_answer = llm.predict(final_prompt)\n",
    "\n",
    "print (llm_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc50c8",
   "metadata": {},
   "source": [
    "Hm, this just isn't good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
